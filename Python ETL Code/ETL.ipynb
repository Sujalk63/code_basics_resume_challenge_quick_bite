{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3504ae",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cc18cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import os # for handling files\n",
    "import pandas as pd # for data cleaning \n",
    "\n",
    "from dotenv import load_dotenv #\n",
    "\n",
    "\n",
    "\n",
    "import sqlalchemy as sal # for connecting to sql database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a9290",
   "metadata": {},
   "source": [
    "### Cleaning and Filtering data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1be44691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_customer: Original rows = 107776, After cleaning = 107776\n",
      "dim_delivery_partner: Original rows = 15000, After cleaning = 15000\n",
      "dim_menu_item: Original rows = 342671, After cleaning = 342671\n",
      "dim_restaurant: Original rows = 19995, After cleaning = 19995\n",
      "fact_delivery_performance: Original rows = 149166, After cleaning = 149166\n",
      "fact_orders: Original rows = 149166, After cleaning = 149166\n",
      "fact_order_items: Original rows = 342994, After cleaning = 342994\n",
      "fact_ratings: Original rows = 68842, After cleaning = 68825\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True) \n",
    "folder_path = os.getenv(\"FOLDER_PATH\")\n",
    "\n",
    "cleaned_tables = {}\n",
    "\n",
    "# Loop through all CSV files in folder and remove duplicates or blanks rows(IF EXISTS!!)\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        table_name = file.replace(\".csv\", \"\")\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        original_rows = len(df)\n",
    "        \n",
    "        # Remove rows where all columns are NaN, if exists\n",
    "        df = df.dropna(how=\"all\")\n",
    "        \n",
    "        # Remove row duplicates, if exists\n",
    "        df_cleaned = df.drop_duplicates()\n",
    "        \n",
    "        print(f\"{table_name}: Original rows = {original_rows}, After cleaning = {len(df_cleaned)}\")\n",
    "        \n",
    "        # Save cleaned DataFrame in dictionary\n",
    "        cleaned_tables[table_name] = df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbf28d",
   "metadata": {},
   "source": [
    "### Converting the  format \"order_timestamp\" from fact_orders table and \"review_timestamp\" from the fact_ratings to datatime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c10e9db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# timestamps are in \"str\" format\n",
    "\n",
    "print(type(cleaned_tables[\"fact_orders\"][\"order_timestamp\"][1]))\n",
    "print(type(cleaned_tables[\"fact_ratings\"][\"review_timestamp\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e408a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tables[\"fact_orders\"][\"order_timestamp\"] = pd.to_datetime(\n",
    "    cleaned_tables[\"fact_orders\"][\"order_timestamp\"], format=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "cleaned_tables[\"fact_ratings\"][\"review_timestamp\"] = pd.to_datetime(\n",
    "    cleaned_tables[\"fact_ratings\"][\"review_timestamp\"], format=\"%d-%m-%Y %H:%M\"\n",
    ").dt.floor(\"min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7fc1c38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147257   2025-09-24 19:52:00\n",
       "Name: order_timestamp, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converted to datetime format\n",
    "\n",
    "cleaned_tables[\"fact_ratings\"][\"review_timestamp\"].sample()\n",
    "cleaned_tables[\"fact_orders\"][\"order_timestamp\"].sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382802cc",
   "metadata": {},
   "source": [
    "### Connecting to MS SQL server and creating Database and Schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9ba9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'quick_bite_database' created successfully.\n",
      "Schema 'quick_bite_schema' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "load_dotenv(override=True) \n",
    "\n",
    "engine_url = os.getenv(\"DB_ENGINE_URL\")  # master DB URL\n",
    "schema_name = \"quick_bite_schema\"\n",
    "database_name = \"quick_bite_database\"\n",
    "\n",
    "# Connect to master to create database if not exists\n",
    "engine_master = create_engine(engine_url)\n",
    "\n",
    "# Check if Schema exists, if not then make it \n",
    "with engine_master.connect().execution_options(isolation_level=\"AUTOCOMMIT\") as conn:\n",
    "    try:\n",
    "        # Ensure database exists\n",
    "        result = conn.execute(text(f\"SELECT * FROM sys.databases WHERE name='{database_name}'\"))\n",
    "        if result.fetchone(): \n",
    "            print(f\"Database '{database_name}' already exists.\")\n",
    "        else:\n",
    "            conn.execute(text(f\"CREATE DATABASE {database_name};\"))\n",
    "            print(f\"Database '{database_name}' created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Failed to create database '{database_name}': {e}\")\n",
    "\n",
    "# Connect to the actual database \"quick_bite_database\"\n",
    "engine_db = create_engine(\n",
    "    f\"mssql+pyodbc://localhost\\\\SQLEXPRESS/{database_name}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    ")\n",
    "\n",
    "# Check if Schema exists, if not then make it \n",
    "with engine_db.connect().execution_options(isolation_level=\"AUTOCOMMIT\") as conn:\n",
    "    try:\n",
    "        result = conn.execute(text(f\"SELECT * FROM sys.schemas WHERE name='{schema_name}'\"))\n",
    "        if result.fetchone():\n",
    "            print(f\"Schema '{schema_name}' already exists.\")\n",
    "        else:\n",
    "            conn.execute(text(f\"EXEC('CREATE SCHEMA {schema_name}')\"))\n",
    "            print(f\"Schema '{schema_name}' created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Failed to create schema '{schema_name}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01201b41",
   "metadata": {},
   "source": [
    "### Load data to SQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0485b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_customer pushed to SQL Server schema 'quick_bite_schema'.\n",
      "dim_delivery_partner pushed to SQL Server schema 'quick_bite_schema'.\n",
      "dim_menu_item pushed to SQL Server schema 'quick_bite_schema'.\n",
      "dim_restaurant pushed to SQL Server schema 'quick_bite_schema'.\n",
      "fact_delivery_performance pushed to SQL Server schema 'quick_bite_schema'.\n",
      "fact_orders pushed to SQL Server schema 'quick_bite_schema'.\n",
      "fact_order_items pushed to SQL Server schema 'quick_bite_schema'.\n",
      "fact_ratings pushed to SQL Server schema 'quick_bite_schema'.\n",
      "ETL Process Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "for table_name, df in cleaned_tables.items():\n",
    "    df.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine_db,\n",
    "        schema=schema_name,\n",
    "        if_exists='append',  # Append new data if table exists\n",
    "        index=False\n",
    "    )\n",
    "    print(f\"{table_name} pushed to SQL Server schema '{schema_name}'.\")\n",
    "\n",
    "print(\"ETL Process Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fa7db",
   "metadata": {},
   "source": [
    "### Optional step, if by mistakenly user ran the script multiple times and then wants to clear the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "96831444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfor table_name in cleaned_tables.keys():\\n    # Fetch the table back from SQL\\n    df_db = pd.read_sql_table(table_name, con=engine_db, schema=schema_name)\\n    \\n    # Remove rows where all columns are NaN\\n    df_db_cleaned = df_db.dropna(how=\"all\")\\n    \\n    # Remove exact duplicate rows (all columns identical)\\n    df_db_cleaned = df_db_cleaned.drop_duplicates()\\n    \\n    # Replace the table in SQL with cleaned data\\n    df_db_cleaned.to_sql(\\n        name=table_name,\\n        con=engine_db,\\n        schema=schema_name,\\n        if_exists=\\'replace\\',  # Replace the existing table\\n        index=False\\n    )\\n    \\n    print(f\"Table \\'{table_name}\\' cleaned and replaced in SQL (NaNs & exact duplicates removed).\")\\n\\n\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "for table_name in cleaned_tables.keys():\n",
    "    # Fetch the table back from SQL\n",
    "    df_db = pd.read_sql_table(table_name, con=engine_db, schema=schema_name)\n",
    "    \n",
    "    # Remove rows where all columns are NaN\n",
    "    df_db_cleaned = df_db.dropna(how=\"all\")\n",
    "    \n",
    "    # Remove exact duplicate rows (all columns identical)\n",
    "    df_db_cleaned = df_db_cleaned.drop_duplicates()\n",
    "    \n",
    "    # Replace the table in SQL with cleaned data\n",
    "    df_db_cleaned.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine_db,\n",
    "        schema=schema_name,\n",
    "        if_exists='replace',  # Replace the existing table\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Table '{table_name}' cleaned and replaced in SQL (NaNs & exact duplicates removed).\")\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
